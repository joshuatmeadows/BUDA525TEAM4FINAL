---
title: 'BUDA 525: Team 4 Final Project'
author: "Ryan Antonini, Danny Germain, Joshua Meadows, Josh Nelson, Bill Robertson"
date: "9/27/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(effects)
library(car)
library(doParallel)
library(carData)
```

# Problem 1



# Problem 2

## Intro

When we are comparing models, there are several methods we can use. Some methods are more simple in that we only look at a calculation based on the specific model output itself and others use sampling methods that compare our prediction between models. The following article will compare six methods by describing how each works, advantages or when to use, and the disadvantages or restrictions of each. 

## R-squared

R-squared, called the coefficient of determination, is used to explain the proportion of the dependent variable that is explainable in predicting the independent (y/response) variable.
  
$$R^2=\frac{SSReg}{SYY}=1-\frac{RSS}{SYY}$$

In order to calculate R-squared, we need a regression model, which produces a prediction line.  The numerator or “explained variance” is calculated by subtracting each of the predicted values from the actual values, square them, and sum across all data points.  The denominator or “total variance” is calculated similarly, but instead of subtracting the actual value from each predicted value, we subtract the average of the actual values from the predicted value.  We do the division and then subtract from 1 to get the R-squared. The output will be a number between 0 and 1, which can be thought of as a percent of the variation that can be explained by the model.

One advantage of evaluating the R-squared is that it is a simple view to get a feel for a single model. Another advantage is that we can explain the variation.  However, we do not necessarily know if our model needs a transformation in the response/predictor variables or if our sampling is sufficient. R-squared should not be used to compare models with a different number of predictors, because adding predictors can only help the R-squared value. This method also should not be used to compare models where the response has been transformed because this number doesn’t account for the scaling that occurs in these instances.

## Anova

Anova or “analysis of variance” or “F-tests” is a way of comparing models that are a subset of one another (I.e. y~x+z vs. y~x). This test produces an F-statistic. If the F-statistic is effectively zero, we conclude the models basically account for the same amount of variance in the residuals, and thus Occam's razor says we should prefer the simpler model. There are a few types of ANOVA test. The most common are type 1 and type 2. The difference between these two is that type 1 depends on the order the predictors are in whereas type 2 does not. If the predictors are orthogonal, these two tests will produce the same values.

## Akaike Information Criterion (AIC)

$$n \log{\left( \frac{RSS_M}{n}  \right)} + 2(p+1)$$

AIC can be used when models we want to compare have the same response (I.e. sqrt(y) vs. sqrt(y)). With these tests, we prefer the model with the lowest score. These quality estimators are nice because they consider the goodness of fit but add a penalty to models with a higher number of predictors. 

## Bayesian Information Criterion (BIC)

$$n \log{\left( \frac{RSS_M}{n}  \right)} + \log{(n)}(p+1)$$

BIC is similar to AIC in that we can compare models that have the same response (I.e. sqrt(y) vs. sqrt(y)). However, BIC penalizes the number of predictors more harshly, so we pay more attention to this score when we want a simpler model for explanation purposes, or we have cause to believe overfitting might be an issue. Again we want to see models with lower scores.  

## K-Fold Cross-Validation

The K-Fold Cross Validation method works by splitting the data into groups.  Once the groups are established, most commonly in five or ten groupings, we loop through the data one time for each grouping.  We hold out one of the groups for prediction and the rest for modeling.  We then calculate the residual sum of squares (RSS) by subtracting the modeled value from the actual value. We run this process for multiple models and then pick the model with the lowest RSS.

This method has some advantages and disadvantages that will impact whether or not to go this route. We can use it to compare models where the response is any transformation of the same variable (I.e. sqrt(y) vs. log(y)). The issue with using K-Fold is that it can be somewhat time-consuming, which is further compounded by the fact that it works best for large data sets. Consequently, these tests should be used sparingly. It is best to use other methods to narrow down the set of candidate models as much as possible before using these methods.

By leveraging parallel computing, we can somewhat reduce the run time of these methods, but it is important to note that we cannot reduce the time complexity of these algorithms. In general, the higher k is, the more refined your comparison is, but this also increases the run-time.

## Random Splitting

Random Splitting is similar to K-Fold Cross-Validation in many ways. They both also allow for model comparison with any transformation of the same response variable. They also both work well with large datasets.  The main difference is that instead of splitting the entire dataset into sections and only testing once through each of those, we can select a much larger number of random samples.  We loop through the dataset randomly sampling the training and validation set.Random splitting produces a much more refined comparison than k-fold cross-validation, but it also takes much more time   



# Problem 3

We are asked to investigate a set of credit data to see what influences credit card balance. We start by looking at a summary of the data.

```{r chunk3_1, echo=FALSE}
#head(Credit)
summary(Credit)
```

We see that $Balance$ can be zero. We keep this information in mind as we proceed. 

Next, we fit a linear model with $Balance$ as the response and all other variables as predictors to see how we should proceed.

```{r chunk3_2,echo=FALSE}
mod3_1 <- lm(Balance~ID+Income+Limit+Rating+Cards+Education+Gender+Student+Married+Ethnicity,data=Credit)
#We start by fitting a model with all predictors to find out how to procede
summary(mod3_1)
```

Next, we fit a model that does not include $Gender$ and $Ethnicity$, do to ethical problems surrounding using these variables as predictors.

```{r chunk3_3,echo=FALSE}
mod3_2 <- lm(Balance~ID+Income+Limit+Rating+Cards+Education+Student+Married,data=Credit)
#Next we fit a model without Gender and Ethnicity since we don't want to use them, and compare them to the model with all predictors
#summary(mod3_2)
anova(mod3_1,mod3_2)
```

The results of our type 2 ANOVA tell us that we can successfully exclude $Gender$ and $Ethnicity$ without reducing the quality of our model. Next, we see if there are any other variables we can exclude based on AIC.

```{r chunk3_4,echo=FALSE}
mod3_3 <- step(mod3_2)
summary(mod3_3)
anova(mod3_1,mod3_3)
#testing to see what we can drop
```

We see that based on AIC and type 2  ANOVA we can use only the predictors $Cards$, $Rating$, $Limit$, $Student$, and $Income$.

Next, we run diagnostics on this model that uses only 5 predictors.


```{r chunk3_5, echo=FALSE }
ncvTest(mod3_3)
par(mfrow=c(2,2))
plot(mod3_3)
#avPlots(mod3_3)
#plot(allEffects(mod3_3))
#Running Diagnostics
```

The results of our NCV test and the residual plot tell us we have NCV. By inspection, we see that this is possibly due to values where $Balance$ is zero. Next, we exclude these values to see if we can get better results.


```{r chunk3_6, echo=FALSE}
Credit2<- Credit[Credit$Balance!=0,]
mod3_4 <- lm(Balance~Income+Limit+Rating+Cards+Student,data=Credit2)
summary(mod3_4)
```

Notice that our R-squared value is 0.998. We would only expect t an R-squared value this high if we have discovered the results of a mathematical calculation. Upon further reflection, we see in the help file for this data set that this is a simulated dataset, so we have likely found something very close to the formula used to create the non-zero values in this dataset.

```{r chunk3_7, echo=FALSE}
ncvTest(mod3_4)
par(mfrow=c(2,2))
plot(mod3_4)
#avPlots(mod3_4)
plot(allEffects(mod3_4))
#Running Diagnostics
```

We notice that $Student$ has a profound effect on $Balance$, with students on average having about a $500 higher balance compared to non-students, when everything else is accounted for. 

Holding all else constant, the following trends are significant at a 0.01 alpha level: 
*For each additional credit card that a person has, their balance goes up, on average, about $25
*For each additional $10,000 a person makes, their balance goes down about $10
*For each additional dollar that a person’s credit limit goes up, their Balance increases $0.34


## Dealing with zero balance

There are 90 values where $Balance$ is zero, which is nearly a fourth of our data. In this section, we will try to find if there are factors that affect whether or not the balance is zero. This section reflects work that was done before we learned that we could exclude zero balance from our analysis and should be thought of as a “bonus section.”

 We create a new variable as our response which takes the value 1 if the person has a balance, and 0 if they do not have a balance. We then create a model that test the 5 relevant predictors above and then we use AIC values to remove unnecessary predictors.


```{r chunk3_8, echo=FALSE}
Credit3<-Credit
Credit3$BalanceF<- as.numeric(Credit3$Balance>0)
mod3_5<- step(lm(BalanceF~Limit+Student+Rating+Cards+Education+Income,data=Credit3))
summary(mod3_5)
```

We find that we only need the 3 variables $Limit$, $Student$, and $Income$ for this model. Next, we test the interactions between these variables and see if we can remove any interaction terms using AIC.

```{r chunk3_9, echo=FALSE}
mod3_7<-step(lm(BalanceF~Student*Limit*Income,data=Credit3))
```

We see that we do not need the interaction between all 3 variables, and we can just use the pairwise interactions.

Notice that what we have done in this section falls outside the normal use of linear regression. We are using the predictors to generate a continuous response, even though our response can only have values zero or one in reality. We must now test whether our model will actually allow us to predict whether or not a person has a balance. To do this, we will use random-splitting, but instead of concerning ourselves with the RSS of the test set, we will predict if the person has a balance in the following way: if the fitted value of the model is greater than 0.5, we predict the person has a balance, and we predict they do not otherwise. We then calculated the percentage we predict correctly over 1000 randomly selected test sets of size 100, using the rest of the data to fit the model.


```{r chunk3_10, echo=FALSE}
mine<-detectCores()
mine<-min(c(max(c(1,mine-1)),5))
cl<-makeCluster(mine)
registerDoParallel(cl)
getDoParWorkers()
library(doRNG)
library(foreach)

foreach(i=1:1000,.combine="+",.options.RNG=623)%dopar% {
  set=sample(1:dim(Credit3)[1],300,replace=FALSE)
	M1<-lm(BalanceF~Student*Limit+Student*Income+Income*Limit,data=Credit3[set,])
	Predict <- predict(M1,newdata=Credit3[-set,])
	myPredict<- ifelse(Predict >0.5,"1","0")
	mytable <- table(Credit3[-set,]$BalanceF,myPredict)
	eff<-sum(diag(mytable))/sum(mytable)
	return(eff)
}

stopCluster(cl)
```

We see that we predict correctly 97.8% of the time. We could first use this model to predict whether or not the person will have a balance, then predict the balance when appropriate, but instead, we try to use the information we learned to fit a model that effectively does all of this in one go by testing $(Student*Limit*Income)*(log(Rating)+cardsF+Student+Limit+Income)$ as the predictors of $Balance$. We then use AIC to remove unnecessary terms, although this analysis is left out for brevity.

```{r chunk3_11, include=FALSE}
mod3_8<-step(lm(Balance~(Student*Limit*Income)*(log(Rating)+Cards+Student+Limit+Income),data=Credit3))
```

```{r chunk3_12, echo=FALSE}
summary(mod3_8)
ncvTest(mod3_8)
par(mfrow=c(2,2))
plot(mod3_8)
```

This resulting model is a little tedious to interpret but deals with the NCV, making it a good forecasting model, but not a good model to explain to a client.

# Problem 4

The Salaries data in the carData package contains information on academic salaries in 2008 and 2009 in a college in the US. A data dictionary can be found in the help file for the data. This data was collected as part of an on-going effort of the college to monitor salary differences between male and female faculty members. We have been asked to investigate the gender gap in the data, but also what other information that may be relevant to admistrators (i.e. salary growth for years of service, discipline based growth, etc). Investigate if there is a gender gap, but also provide insights on other drivers that you may see of salary in the data. Is your model suitable to make offers based on the infromation provided? Explain your reasoning. Provide insights into any other information you find of interest.

We want to investigate the gender gap in the data as well as provide any other drivers that could influence the gap, and determine whether our model of choice is suitable to make offers.


```{r chunk4_1}
library(effects)
library(carData)
library(car)
help(Salaries)
head(Salaries)
summary(Salaries)
dim(Salaries)
```

We have 397 observations with 6 variables. Rank is a factor, with 67 professors being Assistants, 64 being Associate, and 266 are full professors. Discipline is another factor: 181 teach a theoretical discipline, and 216 teach applied. yrs.since.phd has a wide range from 1 year all the way to 56. yrs.service also has a wide range, 0 years (assuming these people's first year was the 08-09 academic calendar year) all the way to 60 years. The 0 could create an issue later on. Sex is a very skewed factor in this data set, 358 are males and only 39 are females. This could create some issues. The nine-month salary is in dollars and ranges from 57,800 - 231,545.

```{r chunk4_2}
t.test(Salaries$salary[Salaries$sex=="Male"], Salaries$salary[Salaries$sex=="Female"], alternative="greater")
```

The t-test = null hypothesis is males makes less than or equal to what females make. So the alternative is that males make more than females. the p-value is less than 0.05. Using a hard cut off, we would reject the null hypothesis that males make more than females. the mean of x (males) is 115090.4. the mean for y (females) is 101002.4. But this is just looking at straight averages, and as we saw above, we have way more male observations than female. And we know that there are other variables that might play a role into the conclusion. 

```{r chunk4_3}
mod1_4<-lm(salary~rank+discipline+yrs.since.phd+yrs.service+sex,data=Salaries)
residualPlot(mod1_4)
par(mfrow=c(2,2))
plot(mod1_4)
ncvTest(mod1_4)
summary(mod1_4)
plot(allEffects(mod1_4))
```

First we will fit a model using salary as our response and the rest of the variables as the predictors. In our residual plot, we see that as the salary increases, so does the variance. Unforuntately this cone-shape is a sign of non-constant variance. We see a similar pattern in the Residuals vs. Fitted plot. The normal Q-Q plot looks ok, has a few points getting away from the line towards the top. The Scale-Location plot has a positive trend showing. The Residuals vs. Leverage plot looks good, nothing is near Cook's distance.

Our ncvTest p-value is close to 0. So we can reject the null hypothesis that the variance of the residuals is constant, and confirm our graphical inference.
Even though we know we have NCV in this model, lets see what the summary says. From here we can see that sex is not a significant factor. But what stands out is that the more years of service, the less money they would make in this model, which does not make any sense. So we will keep trying. 

The allEffects plot shows this negative correlation between yrs.service and salary, along with a high variance as years increase.

```{r chunk4_4}
boxCox(mod1_4)
mod4_4<-lm(I(1/salary)~rank+discipline+yrs.since.phd+yrs.service+sex,data=Salaries)
residualPlot(mod4_4)
ncvTest(mod4_4)
summary(mod4_4)
plot(allEffects(mod4_4))
```

According to boxCox, lambda is close to -1, which tells us to try using the inverse of our repsonse variable. So we try that and check out residuals. We have a very slight u-shaped curve and it looks as if our cone-shape is now just flipped over, but not as bad as before. Our ncvTest p-value is again close to 0. So we can reject the null hypothesis and confirm that we still have NCV. Our summary shows us that our r-squared value is 57%, which we think we can do better than that once we correct the NCV. Our allEffects plots still don't quite make sense. The yrs.service is now corrected (salary increases as does yrs.service) but now it's showing that the higher the rank, the lower the salary. So we know this isn't right. 

```{r chunk4_5}
Salaries$yrs.service.p1 <- Salaries$yrs.service + 1
mod2_4<- lm(log(salary) ~ rank+discipline+sex, weights=1/I(yrs.service.p1), data=Salaries)
residualPlot(mod2_4)
par(mfrow=c(2,2))
plot(mod2_4)
ncvTest(mod2_4)
anova(mod2_4)
AIC(mod2_4)
step(mod2_4)
avPlots(mod2_4)
plot(allEffects(mod2_4))
summary(mod2_4)

mod3_4<- lm(log(salary) ~ rank+discipline, weights=1/I(yrs.service.p1), data=Salaries)
anova(mod3_4, mod2_4)
AIC(mod3_4)
```

In order to make the dataset nicer, we add one to ever observation of yrs.service to get yrs.service.p1. This will allow us to do more with the variable. 

The final model we ended with is using rank + discipline + sex, weighted by yrs.service.p1 to predict log(salary). 

The residual plot looks pretty good. a lot of fitted values are on the same value throughout but it looks like pretty good constant variance. There is a similar pattern in the Residuals vs. Fitted plot. The normal Q-Q plot looks pretty good. The Scale-Location plot looks good, no longer has a trend. The Residuals vs. Leverage plot looks good, nothing is too close to Cook's distance.

Our ncvTest is much better. p-value of 0.94 says we fail to reject the null hypothesis. We do not have enough evidence that this is NCV. So we can assume we have constant variance. 

Anova tells us that each of these variables are significant. The higher the rank, the higher the salary. An applied discipline makes more on average than a theoretical discipline. However, this is also showing that sex matters, meaning if you are male then you make more than if you were female, which is very concerning. 

Our AIC is -268, lower than any other model we tested with log(salary) as the response. Using backwards stepwise to see if we can make any quick changes to our model, it says that doing nothing will give us the lowest AIC, so based on the predictors included in this model, I still shouldn't change anything as it is currently. 

Our avPlots and allEffects plot tell similar stories. Going from Assistant professor to Associate professor is significant, and a nice increase in salary. Going from Associate to Full professor is an even better increase in salary. Teaching in Discipline B (applied) is also an increase in salary from teaching in Discipline A (theoretical). It appears as if there is an increase from going from female to male employee. The whiskers are overlapping a little so lets look to our summary to check these out.

According to our summary, our intercept is 11.18 when everything else is = 0. Since our coefficients are missing female and assitant, we know these are what's being included in our intercept. Our coefficients are giving us valuable insights when using this model. Going from Assistant Professor to ASsociate Professor would result in a 19% increase in average salary. Going from Assistant Professor to Full Professor (which I don't believe can happen) would be a 44% increase in average salary. Teaching an applied discipline instead of a theoretical discipline would be an 11% increase in average salary. And finally, going from female to male would result in an increase of 5% in average salary. Given all of these variables are in the model, all of these variables are significant based on p-value. The fact that sex is considered significant is an issue. Our model r-squared is accounting for 71% of all variance and our model's p-value is significant, so we decide to go with this model. 

Let's see if we can create a better model by removing sex. In mod3_4, all we do is remove sex, and leave everything else the same. 

ANOVA allows us to compare submodels to determine which one is better. mod3_4 is a submodel of mod2_4. Since 'sex' is the only variable we removed, we are basically just comparing what the difference in sex does. Our p-value is significant, so we reject the null hypothesis. We confirm that sex is an important variable. In this instance, simpler is not better, and the model containing sex will be the better model. The F-statistic is telling us that it's better to use variables than to do nothing. We use AIC again to compare the models since our response variables are the same. For mod2_4, our AIC was -268, for mod3_4, our AIC is -263. So it confirms that sex is an important variable and that mod2_4 is the better model to use. 

```{r chunk4_6}

mod2_4<- lm(log(salary) ~ rank+discipline+sex, weights=1/I(yrs.service.p1), data=Salaries)
mod3_4<- lm(log(salary) ~ rank+discipline, weights=1/I(yrs.service.p1), data=Salaries)

library(doParallel)
library(foreach)
library(doRNG) 

mine<-detectCores()
mine<-min(c(max(c(1,mine/2)),5))
cl=makeCluster(mine)
registerDoParallel(cl)
registerDoRNG()
getDoParWorkers()

m1 = 0
m2 = 0

m1 = foreach(i=1:1000,.combine="+",.options.RNG=123)%dopar%{
  set=sample(1:dim(Salaries)[1],300,replace=FALSE)
  m2a<-lm(log(salary) ~ rank+discipline+sex, weights=1/I(yrs.service.p1), data=Salaries[set,])
  sum(((Salaries$salary[-set]-exp(predict(m2a,newdata=Salaries[-set,])))^2))
  }
sum(m1)

m2 = foreach(i=1:1000,.combine="+",.options.RNG=123)%dopar%{
  set=sample(1:dim(Salaries)[1],300,replace=FALSE)
  m3a<-lm(log(salary) ~ rank+discipline, weights=1/I(yrs.service.p1), data=Salaries[set,])
  sum(((Salaries$salary[-set]-exp(predict(m3a,newdata=Salaries[-set,])))^2))
  }
sum(m2)

stopCluster(cl)
```


We can use random splitting to test the models now to make sure the model including sex will be the better predictor. In this method, we are randomly splitting the data, training it, and then testing it however many times we deisgnate, in this case 1000. Predicting over more and more sets will smooth out the average.

We will access the doParallel library and the foreach library to use a for loop while using parallel processing to speed this up. The detectCores() function allows us to find out how many cores are on the machine. Then we divide this by 2, to ensure we are only using half of the available cores. cl will be my cluster and we will registerDoParallel(cl) so I can use the 4 cores. getDoParWorkers verifies the number of cores we are using. 

Now I will set my seed and run the foreach loop. We use registerDoRNG() to ensure the seed will be passed the same way to parallel processing. This is saying we will run through this loop 1000 times, training the data on 300 of the observations, and testing it on the remainder. Then we will calculate the residual sum of squares by comparing how bad our predictions were. But to do this, we must use exp() for our repsonse variable so we are comparing correctly. 

Again, random splitting confirms that we should use mod2_4. The residual sum of squares is smaller.

And then we must always use stopCluster afterwards to make sure we clean up.



